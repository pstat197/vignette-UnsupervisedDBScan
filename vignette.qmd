---
title: "Vignette: Guide to Supervised and Unsupervised Clustering"
format:
  html:
    code-fold: true
    code-tools: true
    number-sections: true
css: bootstrap.css
authors: Aarti Garaye, Josephine Kaminaga, Jimmy Wu, and Nicole Xu
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

Clustering is a fundamental tool in data analysis. It helps uncover latent structure, group similar observations, and identify meaningful patterns in unlabeled data. Because clustering is widely used across domains—from marketing and biology to social sciences and engineering—it is important to understand how different algorithms behave under varying data conditions. This vignette provides a practical, example-driven introduction to four different clustering methods. We deploy DBScan and Kmeans clustering for supervised clustering. For unsupervised we deploy Partitioning Around Medoids (PAM) and Constrained KMeans.

To illustrate these concepts, we use a synthetically generated [personality dataset from Kaggle](https://www.kaggle.com/datasets/miadul/introvert-extrovert-and-ambivert-classification/data). The raw csv file is stored in the data folder of this repository. The dataset is designed to simulate human personality types — Introvert, Extrovert, and Ambivert — based on various behavioral and psychological traits. It containes 20,000 entries and 30 columns, including 29 numerical features representing personality indicators and 1 categorical (3 class) response variable (`personality_type`). 

The goal of this vignette is to provide a clear, accessible guide for performing and interpreting clustering analyses. By walking through multiple algorithms on the same dataset, readers can compare outcomes, understand strengths and limitations, and gain intuition about when and why certain clustering methods are appropriate. This resource is intended to serve as a practical reference for students and practitioners who encounter clustering tasks in applied data projects.

# Dataset Structure

The dataset was obtained from Kaggle. It was synthetically generated using Python and NumPy. Feature distributions were modeled with class-specific means and added noise using a normal distribution to simulate natural variation.

**All feature values are numerical and scaled from 0 (low) to 10 (high).** 

Table: Personality Dataset Feature Descriptions

::: {.panel-body style="max-height:250px; overflow-y:auto;"}

| Column Name               | Description                                           |
|---------------------------|-------------------------------------------------------|
| `personality_type`        | Target variable: Introvert, Extrovert, or Ambivert    |
| `social_energy`           | Tendency to gain energy from social interaction (0–10) |
| `alone_time_preference`   | Comfort with solitude                                 |
| `talkativeness`           | Propensity to engage in conversation                  |
| `deep_reflection`         | Frequency of deep or introspective thinking           |
| `group_comfort`           | Ease in group environments                            |
| `party_liking`            | Enjoyment of parties and social events                |
| `listening_skills`        | Active listening ability                              |
| `empathy`                 | Ability to understand others’ emotions                |
| `creativity`              | Tendency toward creative thinking                     |
| `organization`            | Preference for order, structure, and plans            |
| `leadership`             | Comfort in leading others                              |
| `risk_taking`             | Willingness to take risks                             |
| `public_speaking_comfort` | Comfort level in public speaking situations           |
| `curiosity`               | Interest in learning or exploring                     |
| `routine_preference`      | Preference for routine vs. spontaneity                |
| `excitement_seeking`      | Desire for new and stimulating experiences            |
| `friendliness`            | General social warmth and approachability             |
| `emotional_stability`     | Ability to remain calm and balanced under stress      |
| `planning`                | Tendency to plan ahead                                |
| `spontaneity`             | Acting on impulse or without planning                 |
| `adventurousness`         | Willingness to try new and risky activities           |
| `reading_habit`           | Frequency of reading books or articles                |
| `sports_interest`         | Level of interest in sports or physical activities    |
| `online_social_usage`     | Time spent on social media and online interaction     |
| `travel_desire`           | Interest in travel and exploring new places           |
| `gadget_usage`            | Frequency of gadget or tech device use                |
| `work_style_collaborative`| Preference for teamwork vs. solo work                 |
| `decision_speed`          | How quickly decisions are made                        |
| `stress_handling`         | Ability to manage stress effectively                  |

:::

## Disclaimer

**This is a synthetic dataset created for research, learning, and experimental purposes only. It does not represent real individuals or real-world psychometrics.**

# Exploratory Analysis

## Loading Libraries and Exploring the Data Struction

The code above loads the necessary libraries and the data from the data folder
```{r loading_libs, warning=FALSE, message=FALSE}
library(readr)
library(readxl)
library(visdat)
library(dplyr)
library(caret)
library(ggplot2)
library(knitr)
library(kableExtra)
library(tidyr)
library(tidyverse)
library(cluster)
library(broom)
library(purrr)
library(dbscan)
library(factoextra)

personality <- read.csv("data/raw/personality_synthetic_dataset.csv")
source("Scripts/ckmeans.R")
```

The table shows each variable class.
```{r}
personality %>%
  summarise(across(everything(), ~ class(.x)[1])) %>%
  pivot_longer(everything(), names_to = "column", values_to = "class") %>%
  kable(col.names = c("Column Name", "Class"), caption = "Variable Type in the Dataset") %>%
  scroll_box(height = "200px")
```
There are `r nrow(personality)` number of rows and `r ncol(personality)` columns.

## Missing data
This is synthetically simulated data so we don't expect any discrepancies. We have already check about the missing values and duplicates in the data cleaning process (in the Scripts folder). We will add a vis miss plot here to double check. Furthermore, it is important to note that $0$ values are not missing, they are just values with low scoring for that particular variable. 

```{r missing_data, fig.align='center', fig.cap="As the plot shows above, there are no missing values."}
vis_miss(personality)
```

## Class Balance
There are three classes in our dataset, introvert, extrovert, and ambivert. Although clustering does not use class labels during model fitting, examining class balance is important when labels are available because imbalance can influence cluster structure, distort evaluation metrics, and make interpretation of the resulting clusters less reliable. The graphs below show the class balance

```{r class_balance, fig.cap="The figures show that classes are approximately distributed equally. This is good for later analysis."}
personality_counts <- personality %>%
  count(personality_type)

pie(
  personality_counts$n,
  labels = paste0(personality_counts$personality_type, 
                  " (", round(personality_counts$n / sum(personality_counts$n) * 100, 1), "%)"),
  main = "Personality Type Distribution"
)
```

## Feature Analysis and Variable Importance
We are using four clustering workflows: k-means, PAM, COP-k-means, and DBSCAN. All of these are sensitive to irrelevant or redundant variables. Distance-based methods such as k-means, PAM, and COP-k-means rely on Euclidean distances, so noisy or uninformative features can distort the distance structure and lead to poorly separated or unstable clusters. DBSCAN, while density-based, is also affected by high-dimensional noise because irrelevant variables make it harder to identify dense regions and choose meaningful $\epsilon$ neighborhoods. By identifying which variables contribute most to cluster separation, we reduce dimensionality, improve cluster stability, and ensure that the resulting supervised and unsupervised clusters reflect meaningful structure rather than artifacts of the feature space. 

Recall that all of our predictor variables are numeric and in the summary stats table above we have the means across each variable. We can deploy an ANOVA test to check whether there is any significant difference between the variables. This tells us how much each numeric variable differs between personality groups. In other words the variable that contributes most to the personality type would be the one with highest F-value. 

```{r variable_importance, message=FALSE, warning=FALSE}
# Exclude personality_type (response)
numeric_vars <- personality %>% 
  select(where(is.numeric))

# Compute ANOVA F-statistics for each variable
anova_results <- map_df(names(numeric_vars), function(var) {
  model <- aov(personality[[var]] ~ personality$personality_type)
  tidy(model) %>%
    filter(term != "Residuals") %>%
    mutate(variable = var) %>%
    select(variable, statistic, p.value)
})

# Plot variable importance
anova_results %>%
  arrange(desc(statistic)) %>%
  ggplot(aes(x = reorder(variable, statistic), y = statistic)) +
  geom_col(fill = "mistyrose1", color = "black") +
  coord_flip() +
  labs(title = "Variable Importance (ANOVA F-statistic)",
       x = "Variable",
       y = "F-Statistic") +
  theme_bw()
```

The ANOVA test tells us that the variable that's going to be most distinguishing will be party liking. Variables that don't have much sway are creativity, stress-handling, and emotional-stability. 

# Supervised Clustering
Supervised clustering refers to clustering methods that incorporate constraints while forming clusters. These constraints can be also be user provided. It is used when some label information is available and we want the clusters to respect known relationships while still discovering structure in the data. We deploy two commonly used methods for supervised clustering: PAM and Constrainted KMeans with Background Knowledge to the personality dataset. 

## Partitioning Around Medoids (PAM)
Partitioning Around Medoids (PAM) is a classic clustering algorithm that forms groups by selecting actual data points, **medoids**, as the centers of clusters. PAM chooses representative points that minimize the total distance to other points in their cluster. This makes PAM robust to noise, outliers, and non-spherical cluster shapes. Quoting from the `cluster` package description: "the goal is to find *k* representative objects which minimize the sum of their dissimilarities of the observations to their closest representative object", where the 'representative object' is a medoid. 

The workhorse function for PAM is the `pam()` function from the `cluster` package. This function requires an `x` argument, which is the data matrix or dataframe to perform clustering on, and a `k` argument which represents the number of clusters that we are looking for. There are a number of other optional arguments which enable flexibility for this function. In the function call below, we are specifying `diss = FALSE` to ensure that the function does not think `x` is a dissimilarity matrix, and `metric = "euclidean"` due to the multidimensional nature of our personality dataset. While some implementations of PAM require that the user manually finds medoids, this implementation conducts the search for a suitable initial set of medoids by itself.

```{r fit-pam}
personality <- personality |>
  mutate(
    personality_type = as.factor(personality_type)
  )

pam_clustered <- pam(personality, 3, diss = FALSE,
    metric = "euclidean"
    )
```

The results of the PAM model can be visualized by calling `plot()` upon the PAM model. Below, we can see that when `plot()` is called, we obtain a PCA-reminiscent plot of the personality data along two components that together explain 43.92% of the variability in the data. PAM has identified the three distinct clusters that stand out in this representation. We also obtain a numeric description of the clusters' sizes and numeric widths in the component space.

```{r plot-pam-results}
plot(pam_clustered, main = "PAM Cluster Plot for Personality Data")
```

We can obtain further results by inspecting the fitted elements of the PAM model directly. As seen below, the PAM model returns information about the medoids, their locations, the clusters and their sizes, and some other miscellaneous attributes of the data.

```{r inspect-pam-results}
names(pam_clustered)
```

The most important information from the PAM model can be found by accessing the `medoids`, `clusinfo`, and `silinfo` attributes. The `medoids` attribute contains the values of each medoid along each variable in the data, as seen below.

```{r medoids}
htmltools::div(
style = "height:270px; overflow-y: scroll; border:1px solid #ccc; padding:5px;",
htmltools::HTML(knitr::kable(pam_clustered$medoids, format = "html", caption = "The mean of each variable across the three clusters"))
)
```

The `clusinfo` attribute contains information about the size and dissimilarities of each identified cluster, while the `silinfo` attribute describes the silhouette of the PAM model (a measurement of cluster consistency) and contains multiple smaller attributes - `widths` has information about the designated cluster and silhouette width of each individual data point, `clus.avg.widths` has the average silhouette width of each identified cluster, and `avg.width` is the average width across all clusters. Since `widths` is quite the long attribute, we will skip that and show `clus.avg.widths` instead.

```{r pam_info}
pam_clustered$clusinfo

names(pam_clustered$silinfo)

pam_clustered$silinfo$clus.avg.widths
```

We can also compare the cluster assignments of PAM with the true group labels of the data. The cluster assignments of PAM are found in the `clustering` attribute.

```{r pam_table}
table(pam_clustered$clustering, personality$personality_type)
```

As seen in the table above, the clustering results of PAM are extremely accurate. Ambivert seems like the hardest group to cluster correctly, which makes sense due to its position as a median group between extrovert and introvert.

## Constrained KMeans with Background Knowledge (COP KMeans)
Constrained K-Means (COP K-Means) extends the traditional K-means algorithm by incorporating background knowledge in the form of pairwise constraints. Unlike PAM, COP K-means requires the user to provide constraint sets. For the personality dataset with the outcome varaible as the `personality_type` we can construct constraints that mimic a realistic "semi-supervised" setting: 

- Must-link: A few pairs of people with the same personality_type
      - these shouldn’t be separated by the clustering algorithm
      
- Cannot-Link: A few pairs of people with different personality_type
      - these shouldn’t be placed in the same cluster
      
This uses the ground truth only to form constraints, not as a variable in the clustering.

**COP K-means can be implemented with `conclust`. (Note: this package was removed from CRAN, and we are using a downloaded & archived version of the `ckmeans()` function that is used to perform COP K-means clustering).**

```{r personality_num}
set.seed(11302025)
# keep only numeric features
personality_num <- as.matrix(personality[ , -1])
```

We sample a handful of pairs to create small but meaningful constraint sets. These constraints are only for demonstration — in applied work, constraints come from domain experts. 

```{r fitting_cpkmeans}
ext <- which(personality$personality_type == "Extrovert")
int <- which(personality$personality_type == "Introvert")
amb <- which(personality$personality_type == "Ambivert")

must_link_pairs <- rbind(
  sample(ext, 2),
  sample(int, 2),
  sample(amb, 2)
)
#must_link_pairs

cant_link_pairs <- rbind(
  c(sample(ext, 1), sample(int, 1)),
  c(sample(int, 1), sample(amb, 1)),
  c(sample(ext, 1), sample(amb, 1))
)
#cant_link_pairs

ck_clusters <- ckmeans(
  data = personality_num,
  k = 3,
  mustLink = must_link_pairs,
  cantLink = cant_link_pairs,
  maxIter = 50
)

table(ck_clusters, personality$personality_type)
```

After identifying the must-link pairs and the can't link pairs we fit the COP KMeans and obtain that Cluster 1: 6559 introverts, 14 ambiverts and 0 extroverts is clearly the introvert cluster. Cluster 2 is the Extrovert cluster and Cluster 3 is the Ambivert cluster. This is near-perfect recovery of personality type. COP-KMeans used only 6 constraints total: 3 must-link and 3 cannot-link. And still:

Extroverts is 99.7% correctly clustered
Introverts is 100% correctly clustered
Ambiverts is 99% correctly clustered

We can further visualize this with these plots. Here we are using PC1 and PC2 which are the principle components explaining most of the variability. Note, We are only using PCA for visualization, because this dataset is operating in 30+ dimensional space where plots are impossible to interpret directly. In fact, we did the same with PAM, when we call `pam()` it automatically produces a PCA projection internally.
```{r copkmeans_plots}
# PCA for plotting
pca <- prcomp(personality_num, scale. = TRUE)
pca_df <- as.data.frame(pca$x[,1:2]) |>
  mutate(cluster = factor(ck_clusters))

ggplot(pca_df, aes(PC1, PC2, color = cluster)) +
  geom_point(alpha = 0.6) +
  labs(
    title = "COP K-Means Clustering (PCA Projection)",
    subtitle = "First two principal components",
    color = "Cluster"
  ) +
  theme_bw()
```

For centers of each of the cluster means we can find each means and see how these are different across clusters. In both PAM and COP-KMeans, the cluster center (a medoid for PAM and a mean vector for K-Means) represents the “typical” or most representative profile of individuals in that cluster. Examining these centers allows us to interpret what distinguishes one cluster from another. By comparing centers across clusters, we can characterize the personality patterns each cluster captures and evaluate whether they align with known categories such as introverts, ambiverts, and extroverts. 

```{r copkmeans_table}
cluster_centers <- aggregate(
personality_num,
by = list(cluster = ck_clusters),
FUN = mean
)

htmltools::div(
style = "height:250px; overflow-y: scroll; border:1px solid #ccc; padding:5px;",
htmltools::HTML(knitr::kable(cluster_centers, format = "html", caption = "The mean of each variable across the three clusters"))
)
```

To see how each of the clusters are different than each other we use Silhoutte plot just like we did for PAM. The silhouette plot provides a visual summary of how well each observation fits within its assigned cluster relative to the other clusters. For each data point, the silhouette width ranges from –1 to 1 where values close to 1 indicates the observations are well matched to the clusters they are assigned to. Values close to 0 indicates that the observation is on the borders between the clusters. Negative values means that the point may have been miscalculated, meaning it is closer to another cluster than its assigned one.
```{r copkmeans_sil}
dist_mat <- dist(personality_num)

sil <- silhouette(ck_clusters, dist_mat)

plot(sil, main = "Silhouette Plot for COP K-Means")
```

In the silhouette plot for COP K-Means, each bar represents an individual observation, grouped by cluster and sorted by silhouette width. The height and consistency of the bars show how tight and well-separated each cluster is. As we can see this is very similar to the PAM. Below is the summary table for the COP KMeans.
```{r summ-sil}
summary(sil)
```

Finally, we can cross check this with the truth. We can only do this because we are practicing Supervised Learning.
```{r truth-cpkmeans}
table(ck_clusters, personality$personality_type)
```

As we can see from the table above, the COP KMeans clustering is very close to the ground truth. Ambivert seems like the hardest group to cluster correctly, which makes sense due to its position as a median group between extrovert and introvert. 

# Unsupervised Learning

## DBScan

In this section we will walk through an implementation of DBSCAN, a density based clustering algorithm that groups data points based on how close they are in the feature space. This algorithm differs from other clustering algorithms such as k-means and hierarchical in that it works well with arbitrarily shaped clusters, as well as dealing with outliers and noise. This implementation will be using the `dbscan` R package.

In order to cluster the data, we remove the predictor column since we are using an unsupervised learning method.
```{r}
personality_cluster <- personality %>% select(-personality_type)
```

The two parameters needed for dbscan are epsilon and minimum points. Epsilon is the radius of a cluster around a data point, where if the distance between two points is less than or equal to epsilon, they are considered neighbors. Minimum points is defined as the number of points needed within the epsilon radius to be considered a dense region.

DBSCAN works by categorizing data points into core points (sufficient number of neighbors and within epsilon radius), border points (insufficient number of neighbors but within epsilon radius), and noise points (not belonging to any cluster). It does not require a k number of clusters to be specified in advance and instead iteratively expands clusters from core points by finding density based neighbors.

We first visualize a knee plot to determine the most optimal epsilon value for DBSCAN clustering. According to _Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 19_, for two-dimensional data use a default value of minPts=4. For more than 2 dimensions, define minPts=2*dim. Once you know which minPts to choose, you can determine epsilon by plotting the k-distances with k=minPts. Find the 'knee' in the graph and that corresponding k-distance value is your epsilon value.

```{r}
#setting the value of minimum points and choosing k from the knee plot
set.seed(1)
min_pts = 2*dim(personality_cluster)[2]
kNNdistplot(personality_cluster, k = min_pts)
```

In the above plot, we see that the 'knee' occurs around 7 so we set our epsilon value to 7 and run `dbscan`.
```{r}
set.seed(1)
pers_dbscan <- dbscan(personality_cluster, eps = 7, minPts = min_pts)
pers_dbscan
```

We remove the noise points (data the algorithm decided did not fall into any cluster) and visualize our clusters using the `factoextra` package. This package automatically performs PCA to reduce the dimensionality, and plots the first two principal components.
```{r}
pers_combined <- personality %>% mutate(cluster = pers_dbscan$cluster) %>% filter(cluster != 0)

fviz_cluster(list(clusters = pers_combined[,31], data = pers_combined[,2:30]), geom = "point", ellipse = TRUE, main = "DBScan Clustering w/o noise values")
```



## KMeans

In this part of the project, we apply unsupervised K-means clustering to the *Personality Synthetic Dataset* to investigate whether natural groupings of personality traits emerge without using any labels. Our goals are:

- Identify how many personality clusters exist using common model-selection diagnostics (elbow plot and silhouette analysis).
- Interpret the discovered clusters by summarizing their personality trait profiles.
- Visualize the structure of these clusters using PCA to examine their separability.
- Assess whether the clusters are meaningful, stable, and interpretable in a psychological sense.

Namely, this part of the project answer the following questions: What natural personality clusters exist in the dataset, and what do they represent? Are they meaningful? How stable are they?

```{r}
# Keep only numeric columns of data and remove missing numeric values
dat_num <- personality %>%
  select(where(is.numeric)) %>% 
  drop_na()

dat_scaled <- scale(dat_num)
```

```{r}
# Choose the value of k using elbow method
set.seed(20251126)
max_k <- 10

wss_df <- tibble(
  k = 1:max_k,
  tot_withinss = map_dbl(k, ~kmeans(dat_scaled, centers = .x, nstart = 20)$tot.withinss)
)

# Elbow plot
ggplot(wss_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  geom_point() +
  labs(title = "Elbow plot for K-means",
       x = "Number of clusters (k)",
       y = "Total within-cluster sum of squares")
```
The elbow plot shows the total within-cluster sum of squares decreasing sharply from k = 1 to k = 2, with diminishing improvements for k $\geq$ 3. This indicates that going beyond two clusters adds little additional explanatory power.

```{r}
# Choose the value of k using average silhouette width method
set.seed(20251126)

sil_df <- tibble(
  k = 2:max_k,
  sil_width = map_dbl(k, ~{
    km <- kmeans(dat_scaled, centers = .x, nstart = 20)
    ss <- silhouette(km$cluster, dist(dat_scaled))
    mean(ss[, "sil_width"])
  })
)

# This shows the ideal number of groups is 2
ggplot(sil_df, aes(x = k, y = sil_width)) +
  geom_line() +
  geom_point() +
  labs(title = "Average silhouette width by k",
       x = "Number of clusters (k)",
       y = "Average silhouette width")
```
The silhouette analysis shows the highest average silhouette width at k = 2, with values decreasing steadily at higher k.

Based on the results from both methods, k = 2 is the optimal number of groups, both statistically and structurally. Therefore, we will specify k = 2 for the K-means model fit.

```{r}
# Fitting the unsupervised k-means model with k = 2
set.seed(20251127)

k_best <- 2
km_final <- kmeans(dat_scaled, centers = k_best, nstart = 50)

# Adding assigned clusters to data for plotting results
dat_clusters <- dat_raw %>%
  filter(complete.cases(dat_num)) %>%
  mutate(cluster = factor(km_final$cluster))

# Cluster sizes
cluster_sizes <- dat_clusters %>%
  count(cluster, name = "n")

kable(cluster_sizes)

# Summarise cluster results, revealing two large clustering types
cluster_summary <- dat_clusters %>%
  group_by(cluster) %>%
  summarise(across(where(is.numeric), mean, .names = "mean_{.col}"),
            .groups = "drop")

print(cluster_summary)
```

K-means produced two large and stable clusters consisting of 8,161 individuals and 11,839 individuals correspondingly, giving us confidence that the personality dataset contains two major profiles. This distribution is not perfectly balanced, but both clusters remain large and well-represented.

The cluster summary table reveals two clear personality types. Cluster 1, on the one hand, is featured with high social energy, talkativeness, party liking, friendliness, and excitement seeking, with low alone-time preference and routine preference. With these features observed, we conclude that this cluster resembles extroverted personality.

Cluster 2, on the other hand, is featured with high alone-time preference, deep reflection, and routine preference, with lower scores on social and energetic traits. With these characteristics displayed, we conclude that this cluster resembles introverted personality.

The dataset naturally splits into two psychologically meaningful clusters, corresponding roughly to extroversion vs. introversion. This suggests that K-means has captured real structure that aligns with expectation from general knowledge.

```{r}
# Use PCA to visualize results of clustering along two principal dimensions
pca <- prcomp(dat_scaled, center = TRUE, scale. = TRUE)

pca_df <- as_tibble(pca$x[, 1:2]) %>%
  rename(PC1 = 1, PC2 = 2) %>%
  mutate(cluster = dat_clusters$cluster)

ggplot(pca_df, aes(x = PC1, y = PC2, colour = cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = "K-means clusters visualised in first two PCs") +
  theme_minimal()
```

The PCA plot shows that Cluster 1 and Cluster 2 form two distinct clouds of points. Separation is strongest along PC1, suggesting this component captures the extroversion–introversion axis. This visualization confirms that the clusters are structurally distinct and interpretable within psychological constructs.

Overall, K-means clustering reveals that the *Personality Synthetic Dataset* contains two dominant personality groups. Model-selection diagnostics, trait summaries, and PCA visualization all support this two-cluster structure. These groups correspond closely to introversion and extroversion, one of the most widely studied axes in personality psychology.


# Conclusion

# References